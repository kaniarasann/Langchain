{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGweBeew1KFmZK8BjvLGMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaniarasann/Langchain/blob/main/07.Tools_with_Orchestration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 584,
      "metadata": {
        "id": "_ku_MX_n8RYy"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain langgraph langchain_groq pydantic feedparser beautifulsoup4 langchain-community langchain-exa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "7aFVrbq09PUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel,Field\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dtparser\n",
        "import feedparser\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from typing import List\n",
        "import operator\n",
        "from typing_extensions import Annotated\n",
        "from langchain_exa import ExaSearchResults\n",
        "from langgraph.constants import Send\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "import re"
      ],
      "metadata": {
        "id": "vTUxBNcy9O7E"
      },
      "execution_count": 585,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get top 5 news from the economic times this can be achieved with the help of RSS Feed : https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms"
      ],
      "metadata": {
        "id": "wVgvfPPm8r3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms'"
      ],
      "metadata": {
        "id": "DtnX6xq191-7"
      },
      "execution_count": 586,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class required for converting the Feed to pydantic class\n",
        "class EconomicTime(BaseModel):\n",
        "    title: str =''\n",
        "    link: str = ''\n",
        "    published: datetime =''\n",
        "    content :str =''"
      ],
      "metadata": {
        "id": "it4L88C78gcw"
      },
      "execution_count": 587,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feed parse the Url add the nessecary package\n",
        "feeds = feedparser.parse(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEdCRu3W9hyY",
        "outputId": "2fcada8c-da74-40c8-9e93-a1f628eaa98c"
      },
      "execution_count": 588,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate through the first 5 feed\n",
        "economic_times:list[EconomicTime] = []\n",
        "for feed in feeds.entries[:5]:\n",
        "    economic_times.append(EconomicTime(\n",
        "        title = feed.title,\n",
        "        link = feed.link,\n",
        "        published = dtparser.parse(feed.published)\n",
        "\n",
        "    ))\n",
        ""
      ],
      "metadata": {
        "id": "d4QXS9rI94m2"
      },
      "execution_count": 589,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the content by using beautiful soup\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "for news in economic_times:\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(news.link, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    article_div = soup.find(\"article\")\n",
        "    article_text = article_div.get_text(strip=True)\n",
        "    news.content = article_text"
      ],
      "metadata": {
        "id": "npSW6JNE95MF"
      },
      "execution_count": 590,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create LLM\n",
        "groq_key = userdata.get('groq_key')\n",
        "llm = ChatGroq(api_key=groq_key, model=\"qwen/qwen3-32b\",max_retries=5)\n",
        "llm_reasoning = ChatGroq(api_key=groq_key, model=\"meta-llama/llama-4-scout-17b-16e-instruct\",max_retries=5)\n",
        "llm_deep_seek = ChatGroq(api_key=groq_key, model=\"deepseek-r1-distill-llama-70b\",max_retries=5)\n",
        "search_tool = ExaSearchResults(exa_api_key= userdata.get('exa_api_key'))\n"
      ],
      "metadata": {
        "id": "HDs6EXEm_Ovq"
      },
      "execution_count": 591,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stock(BaseModel):\n",
        "    name: str = Field(description=\"Name of the stock\")\n",
        "    news_abstract: str = Field(description=\"Abstract of the news article related to the stock\")"
      ],
      "metadata": {
        "id": "NyEaazA-_md-"
      },
      "execution_count": 592,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_stock_news = llm.with_structured_output(Stock)"
      ],
      "metadata": {
        "id": "tVTLlrcXBOBz"
      },
      "execution_count": 593,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_news:list[Stock] = []\n",
        "def abstract_stock_new(news):\n",
        "  prompt_stock = PromptTemplate.from_template(\"Following is the news:{news}. provide the company name and abstract news \").format(news=news.content)\n",
        "  return llm_stock_news.invoke(prompt_stock)\n",
        "\n",
        "for news in economic_times[:1]:\n",
        "  stock_news.append(abstract_stock_new(news))\n"
      ],
      "metadata": {
        "id": "h-s_cSn5_9eM"
      },
      "execution_count": 594,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create State\n",
        "class State(BaseModel):\n",
        "    company_name: str = Field(description=\"Name of the company\")\n",
        "    news_abstract: str = Field(description=\"Abstract of the news article related to the company\")\n",
        "    queries:List[str] = []\n",
        "    completed_queries: Annotated[List[str], operator.add] = []\n",
        "    final_result: str = ''\n",
        "\n",
        "class Query(BaseModel):\n",
        "   query:str = ''\n",
        "\n",
        "class Queries(BaseModel):\n",
        "  queries:List[str] = []"
      ],
      "metadata": {
        "id": "hq4zmR3DB3ZN"
      },
      "execution_count": 595,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Worker State\n",
        "class WorkerState(BaseModel):\n",
        "    queries: str = ''\n",
        "    completed_queries: Annotated[List[str], operator.add] = []"
      ],
      "metadata": {
        "id": "LYQiPU5zB3XB"
      },
      "execution_count": 596,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_queries = llm_reasoning.with_structured_output(Queries)\n",
        "def orchestrator(state:State):\n",
        "  promot_template = PromptTemplate.from_template(\"Your are Stock Market expert. Based on the following news list out the impacts in short / long:{news}\").format(news=state.news_abstract)\n",
        "  res = llm_queries.invoke(promot_template)\n",
        "  return {'queries':res.queries}\n",
        "\n",
        "def llm_call(query):\n",
        "    \"\"\"Worker queries web for queries\"\"\"\n",
        "    search_results = search_tool._run(\n",
        "          query=query['query'],\n",
        "          num_results=1,\n",
        "          text_contents_options=True,\n",
        "          highlights=True,\n",
        "      )\n",
        "    prompt_stock = PromptTemplate.from_template(\"Following is the news:{news}. provide the abstract\").format(news=search_results)\n",
        "    res_abs = llm.invoke(prompt_stock)\n",
        "    return {'completed_queries':[remove_think_blocks(res_abs.content)]} # Wrap the content in a list\n",
        "\n",
        "def assign_workers(state: State):\n",
        "    return [Send(\"llm_call\", {\"query\": s}) for s in state.queries]\n",
        "\n",
        "def synthesizer(state: State):\n",
        "    body = \"\".join(state.completed_queries)\n",
        "    promot_template = PromptTemplate.from_template(\"Your are Stock Market expert.Provide an abstract on the following news and do a analysis:{news}\").format(news=body)\n",
        "    res = llm_deep_seek.invoke(promot_template)\n",
        "    return {\"final_result\": remove_think_blocks(res.content)}\n",
        "\n",
        "def remove_think_blocks(text: str) -> str:\n",
        "    # This pattern removes everything between <think> and </think>, including the tags\n",
        "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n"
      ],
      "metadata": {
        "id": "KtJrxTWGD1Zy"
      },
      "execution_count": 597,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(State)\n",
        "graph.add_node(\"orchestration\", orchestrator)\n",
        "graph.add_node(\"llm_call\", llm_call)\n",
        "graph.add_node(\"synthesizer\", synthesizer)\n",
        "\n",
        "graph.add_edge(START, \"orchestration\")\n",
        "graph.add_conditional_edges(\"orchestration\", assign_workers, [\"llm_call\"])\n",
        "graph.add_edge(\"llm_call\", \"synthesizer\")\n",
        "graph.add_edge(\"synthesizer\", END)\n",
        "\n",
        "builder = graph.compile()"
      ],
      "metadata": {
        "id": "MrPuJTdNiPiC"
      },
      "execution_count": 598,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state = State(company_name=\"TCS\",news_abstract=)\n",
        "selector = stock_news[0]\n",
        "state = State(company_name=selector.name,news_abstract=selector.news_abstract)"
      ],
      "metadata": {
        "id": "_pzbCPCHzeAa"
      },
      "execution_count": 599,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_date = builder.invoke(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "qvlg1k9C0v3D",
        "outputId": "5f81e385-3d32-42bb-d19b-d129c4237544"
      },
      "execution_count": 600,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `qwen/qwen3-32b` in organization `org_01jzb4yhhpe01ampb9qw2edk5s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9360, Requested 3498. Please try again in 1m8.584999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-600-509047671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[1;32m   2842\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2844\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2845\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2532\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2534\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2535\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-597-3102872990.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     14\u001b[0m       )\n\u001b[1;32m     15\u001b[0m     \u001b[0mprompt_stock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Following is the news:{news}. provide the abstract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mres_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_stock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'completed_queries'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremove_think_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_abs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# Wrap the content in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         return cast(\n\u001b[1;32m    377\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    379\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    962\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 results.append(\n\u001b[0;32m--> 782\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1029\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         }\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         )\n\u001b[0;32m-> 1232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `qwen/qwen3-32b` in organization `org_01jzb4yhhpe01ampb9qw2edk5s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9360, Requested 3498. Please try again in 1m8.584999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_date"
      ],
      "metadata": {
        "id": "aFwdjxpd1ANB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHf1wMwU8bCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}